{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbO5bt9yLXj9u8noHqAWYg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmethan96/Neural-netweok/blob/main/Predict%20car%20prices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split"
      ],
      "metadata": {
        "id": "_t_nB-VrVY4K"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DATA_FILENAME = \"car_models.csv\"\n",
        "dataframe_raw = pd.read_csv(DATA_FILENAME)\n",
        "dataframe_raw.head()"
      ],
      "metadata": {
        "id": "Nr2d6bhsVc8o",
        "outputId": "6b74146e-47be-40ed-891e-254b696e01bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Car_Name  Year  Selling_Price  Present_Price  Kms_Driven Fuel_Type  \\\n",
              "0     ritz  2014           3.35           5.59       27000    Petrol   \n",
              "1      sx4  2013           4.75           9.54       43000    Diesel   \n",
              "2     ciaz  2017           7.25           9.85        6900    Petrol   \n",
              "3  wagon r  2011           2.85           4.15        5200    Petrol   \n",
              "4    swift  2014           4.60           6.87       42450    Diesel   \n",
              "\n",
              "  Seller_Type Transmission  Owner  \n",
              "0      Dealer       Manual      0  \n",
              "1      Dealer       Manual      0  \n",
              "2      Dealer       Manual      0  \n",
              "3      Dealer       Manual      0  \n",
              "4      Dealer       Manual      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-05959c2d-15ee-44cb-9350-515abc1f39ef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Car_Name</th>\n",
              "      <th>Year</th>\n",
              "      <th>Selling_Price</th>\n",
              "      <th>Present_Price</th>\n",
              "      <th>Kms_Driven</th>\n",
              "      <th>Fuel_Type</th>\n",
              "      <th>Seller_Type</th>\n",
              "      <th>Transmission</th>\n",
              "      <th>Owner</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ritz</td>\n",
              "      <td>2014</td>\n",
              "      <td>3.35</td>\n",
              "      <td>5.59</td>\n",
              "      <td>27000</td>\n",
              "      <td>Petrol</td>\n",
              "      <td>Dealer</td>\n",
              "      <td>Manual</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sx4</td>\n",
              "      <td>2013</td>\n",
              "      <td>4.75</td>\n",
              "      <td>9.54</td>\n",
              "      <td>43000</td>\n",
              "      <td>Diesel</td>\n",
              "      <td>Dealer</td>\n",
              "      <td>Manual</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ciaz</td>\n",
              "      <td>2017</td>\n",
              "      <td>7.25</td>\n",
              "      <td>9.85</td>\n",
              "      <td>6900</td>\n",
              "      <td>Petrol</td>\n",
              "      <td>Dealer</td>\n",
              "      <td>Manual</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wagon r</td>\n",
              "      <td>2011</td>\n",
              "      <td>2.85</td>\n",
              "      <td>4.15</td>\n",
              "      <td>5200</td>\n",
              "      <td>Petrol</td>\n",
              "      <td>Dealer</td>\n",
              "      <td>Manual</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>swift</td>\n",
              "      <td>2014</td>\n",
              "      <td>4.60</td>\n",
              "      <td>6.87</td>\n",
              "      <td>42450</td>\n",
              "      <td>Diesel</td>\n",
              "      <td>Dealer</td>\n",
              "      <td>Manual</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05959c2d-15ee-44cb-9350-515abc1f39ef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-05959c2d-15ee-44cb-9350-515abc1f39ef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-05959c2d-15ee-44cb-9350-515abc1f39ef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "your_name = \"Aman Kharwal\" # at least 5 characters\n",
        "def customize_dataset(dataframe_raw, rand_str):\n",
        "    dataframe = dataframe_raw.copy(deep=True)\n",
        "    # drop some rows\n",
        "    dataframe = dataframe.sample(int(0.95*len(dataframe)), random_state=int(ord(rand_str[0])))\n",
        "    # scale input\n",
        "    dataframe.Year = dataframe.Year * ord(rand_str[1])/100.\n",
        "    # scale target\n",
        "    dataframe.Selling_Price = dataframe.Selling_Price * ord(rand_str[2])/100.\n",
        "    # drop column\n",
        "    if ord(rand_str[3]) % 2 == 1:\n",
        "        dataframe = dataframe.drop(['Car_Name'], axis=1)\n",
        "    return dataframe\n",
        "\n",
        "dataframe = customize_dataset(dataframe_raw, your_name)\n",
        "dataframe.head()\n"
      ],
      "metadata": {
        "id": "H4xqL31cVhVI",
        "outputId": "6b9b27fa-df99-4a19-9074-d06d7f7c8c66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Car_Name     Year  Selling_Price  Present_Price  Kms_Driven  \\\n",
              "2                    ciaz  2198.53         7.0325           9.85        6900   \n",
              "128  Honda CB Hornet 160R  2198.53         0.7760           0.87        3000   \n",
              "286                  jazz  2197.44         5.4805           7.90       28569   \n",
              "272                  city  2196.35         7.2750          10.00       27600   \n",
              "205             grand i10  2197.44         5.0925           5.70        3493   \n",
              "\n",
              "    Fuel_Type Seller_Type Transmission  Owner  \n",
              "2      Petrol      Dealer       Manual      0  \n",
              "128    Petrol  Individual       Manual      0  \n",
              "286    Petrol      Dealer       Manual      0  \n",
              "272    Petrol      Dealer       Manual      0  \n",
              "205    Petrol      Dealer       Manual      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d120d21-860f-4d5b-9bb3-8a69540a9e06\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Car_Name</th>\n",
              "      <th>Year</th>\n",
              "      <th>Selling_Price</th>\n",
              "      <th>Present_Price</th>\n",
              "      <th>Kms_Driven</th>\n",
              "      <th>Fuel_Type</th>\n",
              "      <th>Seller_Type</th>\n",
              "      <th>Transmission</th>\n",
              "      <th>Owner</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ciaz</td>\n",
              "      <td>2198.53</td>\n",
              "      <td>7.0325</td>\n",
              "      <td>9.85</td>\n",
              "      <td>6900</td>\n",
              "      <td>Petrol</td>\n",
              "      <td>Dealer</td>\n",
              "      <td>Manual</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>Honda CB Hornet 160R</td>\n",
              "      <td>2198.53</td>\n",
              "      <td>0.7760</td>\n",
              "      <td>0.87</td>\n",
              "      <td>3000</td>\n",
              "      <td>Petrol</td>\n",
              "      <td>Individual</td>\n",
              "      <td>Manual</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>jazz</td>\n",
              "      <td>2197.44</td>\n",
              "      <td>5.4805</td>\n",
              "      <td>7.90</td>\n",
              "      <td>28569</td>\n",
              "      <td>Petrol</td>\n",
              "      <td>Dealer</td>\n",
              "      <td>Manual</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>city</td>\n",
              "      <td>2196.35</td>\n",
              "      <td>7.2750</td>\n",
              "      <td>10.00</td>\n",
              "      <td>27600</td>\n",
              "      <td>Petrol</td>\n",
              "      <td>Dealer</td>\n",
              "      <td>Manual</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>grand i10</td>\n",
              "      <td>2197.44</td>\n",
              "      <td>5.0925</td>\n",
              "      <td>5.70</td>\n",
              "      <td>3493</td>\n",
              "      <td>Petrol</td>\n",
              "      <td>Dealer</td>\n",
              "      <td>Manual</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d120d21-860f-4d5b-9bb3-8a69540a9e06')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9d120d21-860f-4d5b-9bb3-8a69540a9e06 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9d120d21-860f-4d5b-9bb3-8a69540a9e06');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_cols = [\"Year\",\"Present_Price\",\"Kms_Driven\",\"Owner\"]\n",
        "categorical_cols = [\"Fuel_Type\",\"Seller_Type\",\"Transmission\"]\n",
        "output_cols = [\"Selling_Price\"]"
      ],
      "metadata": {
        "id": "RXIgVhqtVjGk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataframe_to_arrays(dataframe):\n",
        "    # Make a copy of the original dataframe\n",
        "    dataframe1 = dataframe.copy(deep=True)\n",
        "    # Convert non-numeric categorical columns to numbers\n",
        "    for col in categorical_cols:\n",
        "        dataframe1[col] = dataframe1[col].astype('category').cat.codes\n",
        "    # Extract input & outupts as numpy arrays\n",
        "    inputs_array = dataframe1[input_cols].to_numpy()\n",
        "    targets_array = dataframe1[output_cols].to_numpy()\n",
        "    return inputs_array, targets_array\n",
        "\n",
        "inputs_array, targets_array = dataframe_to_arrays(dataframe)\n",
        "inputs_array, targets_array"
      ],
      "metadata": {
        "id": "AwROAJRvVqpL",
        "outputId": "6460bcb6-80af-4879-b98d-3c231d34f5d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[2.19853e+03, 9.85000e+00, 6.90000e+03, 0.00000e+00],\n",
              "        [2.19853e+03, 8.70000e-01, 3.00000e+03, 0.00000e+00],\n",
              "        [2.19744e+03, 7.90000e+00, 2.85690e+04, 0.00000e+00],\n",
              "        ...,\n",
              "        [2.19853e+03, 3.62300e+01, 6.00000e+03, 0.00000e+00],\n",
              "        [2.18872e+03, 5.20000e-01, 5.00000e+05, 0.00000e+00],\n",
              "        [2.19635e+03, 8.40000e-01, 5.80000e+04, 0.00000e+00]]),\n",
              " array([[ 7.0325],\n",
              "        [ 0.776 ],\n",
              "        [ 5.4805],\n",
              "        [ 7.275 ],\n",
              "        [ 5.0925],\n",
              "        [ 0.4656],\n",
              "        [ 2.7645],\n",
              "        [ 0.3395],\n",
              "        [ 1.6975],\n",
              "        [ 4.8015],\n",
              "        [ 8.9725],\n",
              "        [ 5.141 ],\n",
              "        [ 3.783 ],\n",
              "        [ 8.827 ],\n",
              "        [ 4.268 ],\n",
              "        [ 3.3853],\n",
              "        [19.1575],\n",
              "        [ 7.0325],\n",
              "        [10.9125],\n",
              "        [12.513 ],\n",
              "        [14.2881],\n",
              "        [ 4.753 ],\n",
              "        [ 5.0925],\n",
              "        [ 3.1525],\n",
              "        [ 7.2265],\n",
              "        [ 0.5335],\n",
              "        [ 2.813 ],\n",
              "        [ 0.3007],\n",
              "        [ 0.2619],\n",
              "        [ 7.178 ],\n",
              "        [ 0.582 ],\n",
              "        [ 5.9655],\n",
              "        [ 2.7645],\n",
              "        [ 8.148 ],\n",
              "        [ 0.485 ],\n",
              "        [ 7.275 ],\n",
              "        [ 0.9215],\n",
              "        [ 5.7715],\n",
              "        [ 4.365 ],\n",
              "        [ 0.485 ],\n",
              "        [ 4.365 ],\n",
              "        [ 2.91  ],\n",
              "        [ 0.4365],\n",
              "        [ 0.291 ],\n",
              "        [ 4.6075],\n",
              "        [ 0.2425],\n",
              "        [ 0.4074],\n",
              "        [ 3.977 ],\n",
              "        [ 4.365 ],\n",
              "        [ 0.4365],\n",
              "        [ 0.4656],\n",
              "        [ 1.0185],\n",
              "        [ 2.813 ],\n",
              "        [19.3903],\n",
              "        [ 0.4365],\n",
              "        [ 0.5044],\n",
              "        [ 3.686 ],\n",
              "        [ 5.335 ],\n",
              "        [ 1.164 ],\n",
              "        [ 0.097 ],\n",
              "        [ 4.753 ],\n",
              "        [ 0.1164],\n",
              "        [ 5.917 ],\n",
              "        [ 8.2935],\n",
              "        [ 0.388 ],\n",
              "        [ 0.194 ],\n",
              "        [ 0.388 ],\n",
              "        [ 5.0925],\n",
              "        [ 8.7203],\n",
              "        [ 6.5475],\n",
              "        [ 2.4735],\n",
              "        [ 6.305 ],\n",
              "        [ 0.485 ],\n",
              "        [ 0.1552],\n",
              "        [ 0.7275],\n",
              "        [ 0.388 ],\n",
              "        [ 4.656 ],\n",
              "        [ 1.164 ],\n",
              "        [ 4.656 ],\n",
              "        [ 3.6375],\n",
              "        [ 8.9725],\n",
              "        [ 5.723 ],\n",
              "        [ 2.425 ],\n",
              "        [ 4.7045],\n",
              "        [ 0.4947],\n",
              "        [ 1.3095],\n",
              "        [ 1.2125],\n",
              "        [ 5.335 ],\n",
              "        [ 2.2795],\n",
              "        [11.3975],\n",
              "        [ 8.3905],\n",
              "        [ 0.1455],\n",
              "        [ 0.485 ],\n",
              "        [ 9.9425],\n",
              "        [ 2.1825],\n",
              "        [ 3.88  ],\n",
              "        [ 3.88  ],\n",
              "        [12.125 ],\n",
              "        [ 3.4047],\n",
              "        [ 4.6075],\n",
              "        [ 0.3686],\n",
              "        [ 0.2425],\n",
              "        [ 8.4875],\n",
              "        [10.573 ],\n",
              "        [ 7.2265],\n",
              "        [ 0.582 ],\n",
              "        [ 0.582 ],\n",
              "        [ 0.6305],\n",
              "        [ 0.194 ],\n",
              "        [ 1.0185],\n",
              "        [ 7.275 ],\n",
              "        [ 1.3095],\n",
              "        [ 0.194 ],\n",
              "        [ 1.4065],\n",
              "        [ 4.2195],\n",
              "        [ 2.619 ],\n",
              "        [ 9.3605],\n",
              "        [ 3.3465],\n",
              "        [ 4.365 ],\n",
              "        [ 5.335 ],\n",
              "        [ 3.395 ],\n",
              "        [ 0.194 ],\n",
              "        [ 3.8315],\n",
              "        [ 1.1155],\n",
              "        [ 4.365 ],\n",
              "        [ 0.582 ],\n",
              "        [ 0.291 ],\n",
              "        [ 3.977 ],\n",
              "        [ 1.1155],\n",
              "        [17.46  ],\n",
              "        [ 0.873 ],\n",
              "        [ 5.82  ],\n",
              "        [ 2.5705],\n",
              "        [ 4.268 ],\n",
              "        [ 4.365 ],\n",
              "        [ 0.4365],\n",
              "        [ 6.984 ],\n",
              "        [ 0.4365],\n",
              "        [ 8.0025],\n",
              "        [ 3.5405],\n",
              "        [ 2.91  ],\n",
              "        [ 2.1825],\n",
              "        [ 2.1825],\n",
              "        [ 0.4656],\n",
              "        [ 9.409 ],\n",
              "        [ 1.0767],\n",
              "        [ 3.0555],\n",
              "        [ 7.5175],\n",
              "        [ 8.9725],\n",
              "        [ 5.5775],\n",
              "        [ 9.8067],\n",
              "        [ 0.6305],\n",
              "        [ 4.462 ],\n",
              "        [ 6.402 ],\n",
              "        [ 2.5705],\n",
              "        [ 3.8315],\n",
              "        [ 4.6075],\n",
              "        [ 4.9567],\n",
              "        [ 2.6675],\n",
              "        [ 0.7275],\n",
              "        [ 8.8755],\n",
              "        [ 6.7415],\n",
              "        [ 3.1525],\n",
              "        [ 1.3095],\n",
              "        [ 4.6075],\n",
              "        [ 6.6445],\n",
              "        [ 0.1746],\n",
              "        [22.31  ],\n",
              "        [ 0.3395],\n",
              "        [ 1.8915],\n",
              "        [ 3.007 ],\n",
              "        [ 0.4074],\n",
              "        [ 6.0625],\n",
              "        [ 0.7275],\n",
              "        [ 5.238 ],\n",
              "        [ 1.1155],\n",
              "        [ 7.2265],\n",
              "        [18.1875],\n",
              "        [ 7.5175],\n",
              "        [15.52  ],\n",
              "        [ 8.148 ],\n",
              "        [ 5.82  ],\n",
              "        [ 5.335 ],\n",
              "        [ 5.0925],\n",
              "        [ 1.0185],\n",
              "        [ 0.582 ],\n",
              "        [ 2.91  ],\n",
              "        [ 5.1895],\n",
              "        [ 2.8615],\n",
              "        [ 1.067 ],\n",
              "        [ 3.88  ],\n",
              "        [ 0.6984],\n",
              "        [ 0.4656],\n",
              "        [13.8225],\n",
              "        [ 0.6305],\n",
              "        [ 0.388 ],\n",
              "        [ 4.85  ],\n",
              "        [ 3.783 ],\n",
              "        [ 5.141 ],\n",
              "        [ 1.0185],\n",
              "        [ 2.7645],\n",
              "        [ 6.499 ],\n",
              "        [ 4.6075],\n",
              "        [ 2.037 ],\n",
              "        [14.065 ],\n",
              "        [ 3.007 ],\n",
              "        [ 6.2565],\n",
              "        [ 3.395 ],\n",
              "        [ 5.82  ],\n",
              "        [ 0.582 ],\n",
              "        [ 3.492 ],\n",
              "        [ 2.8615],\n",
              "        [ 0.485 ],\n",
              "        [ 8.0995],\n",
              "        [ 4.268 ],\n",
              "        [ 4.5105],\n",
              "        [ 5.0925],\n",
              "        [ 0.291 ],\n",
              "        [ 0.3395],\n",
              "        [ 5.5775],\n",
              "        [ 2.4735],\n",
              "        [ 0.2425],\n",
              "        [ 7.663 ],\n",
              "        [ 1.6005],\n",
              "        [ 1.1155],\n",
              "        [ 5.335 ],\n",
              "        [ 0.4365],\n",
              "        [ 0.7566],\n",
              "        [ 5.82  ],\n",
              "        [ 6.0625],\n",
              "        [10.9125],\n",
              "        [ 0.582 ],\n",
              "        [ 3.1525],\n",
              "        [22.31  ],\n",
              "        [ 4.9955],\n",
              "        [ 1.164 ],\n",
              "        [ 0.873 ],\n",
              "        [ 1.8915],\n",
              "        [ 3.2495],\n",
              "        [ 0.3686],\n",
              "        [ 5.626 ],\n",
              "        [ 1.2125],\n",
              "        [ 1.455 ],\n",
              "        [ 0.2425],\n",
              "        [ 0.4365],\n",
              "        [ 6.305 ],\n",
              "        [ 3.6375],\n",
              "        [33.95  ],\n",
              "        [ 0.194 ],\n",
              "        [ 5.0925],\n",
              "        [ 3.2495],\n",
              "        [11.155 ],\n",
              "        [ 0.7275],\n",
              "        [ 3.007 ],\n",
              "        [22.31  ],\n",
              "        [ 4.6075],\n",
              "        [20.1275],\n",
              "        [ 6.208 ],\n",
              "        [14.453 ],\n",
              "        [ 5.238 ],\n",
              "        [ 5.044 ],\n",
              "        [ 2.6675],\n",
              "        [ 5.0925],\n",
              "        [ 1.067 ],\n",
              "        [11.1065],\n",
              "        [ 4.8015],\n",
              "        [10.9125],\n",
              "        [ 2.5705],\n",
              "        [ 2.425 ],\n",
              "        [ 5.6745],\n",
              "        [ 0.194 ],\n",
              "        [ 5.7715],\n",
              "        [ 4.365 ],\n",
              "        [ 0.4365],\n",
              "        [ 0.97  ],\n",
              "        [ 2.91  ],\n",
              "        [ 5.6745],\n",
              "        [ 0.3395],\n",
              "        [ 3.88  ],\n",
              "        [ 1.649 ],\n",
              "        [ 7.5175],\n",
              "        [ 6.8385],\n",
              "        [32.01  ],\n",
              "        [ 0.1649],\n",
              "        [ 0.5335]]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.Tensor(inputs_array)\n",
        "targets = torch.Tensor(targets_array)\n",
        "\n",
        "dataset = TensorDataset(inputs, targets)\n",
        "train_ds, val_ds = random_split(dataset, [228, 57])\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size)"
      ],
      "metadata": {
        "id": "7YrXDArvVsNo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_size = len(input_cols)\n",
        "output_size = len(output_cols)\n",
        "\n",
        "class CarsModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, output_size)                  # fill this (hint: use input_size & output_size defined above)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        out = self.linear(xb)                          # fill this\n",
        "        return out\n",
        "    \n",
        "    def training_step(self, batch):\n",
        "        inputs, targets = batch \n",
        "        # Generate predictions\n",
        "        out = self(inputs)          \n",
        "        # Calcuate loss\n",
        "        loss = F.l1_loss(out, targets)                         # fill this\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        inputs, targets = batch\n",
        "        # Generate predictions\n",
        "        out = self(inputs)\n",
        "        # Calculate loss\n",
        "        loss = F.l1_loss(out, targets)                           # fill this    \n",
        "        return {'val_loss': loss.detach()}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        return {'val_loss': epoch_loss.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result, num_epochs):\n",
        "        # Print result every 20th epoch\n",
        "        if (epoch+1) % 20 == 0 or epoch == num_epochs-1:\n",
        "            print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch+1, result['val_loss']))\n",
        "            \n",
        "model = CarsModel()\n",
        "\n",
        "list(model.parameters())"
      ],
      "metadata": {
        "id": "BQ5RvtfAVu_N",
        "outputId": "2cc2dec6-a29d-4c24-8495-e72e1310c6ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.1366, -0.0193,  0.4636, -0.4758]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.3747], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval algorithm\n",
        "def evaluate(model, val_loader):\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "# Fitting algorithm\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        model.epoch_end(epoch, result, epochs)\n",
        "        history.append(result)\n",
        "    return history\n",
        "\n",
        "# Check the initial value that val_loss have\n",
        "result = evaluate(model, val_loader)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "LHDaAfkJV5s4",
        "outputId": "600d44af-8987-4855-99df-39f99e0cc70e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'val_loss': 14172.92578125}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start with the Fitting\n",
        "epochs = 90\n",
        "lr = 1e-8\n",
        "history1 = fit(epochs, lr, model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "Bv9JQvLhV8VD",
        "outputId": "1bc2dcb1-7a86-465c-e0cf-026b9b681de1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20], val_loss: 13681.3135\n",
            "Epoch [40], val_loss: 13192.2539\n",
            "Epoch [60], val_loss: 12701.5459\n",
            "Epoch [80], val_loss: 12210.7305\n",
            "Epoch [90], val_loss: 11966.6855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train repeatdly until have a 'good' val_loss\n",
        "epochs = 9000\n",
        "lr = 1e-9\n",
        "history1 = fit(epochs, lr, model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "VhKp8haBWCcX",
        "outputId": "e2e82085-cede-4a9c-b2f4-7636d88ef2cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20], val_loss: 186.9152\n",
            "Epoch [40], val_loss: 186.8589\n",
            "Epoch [60], val_loss: 186.8030\n",
            "Epoch [80], val_loss: 186.7506\n",
            "Epoch [100], val_loss: 186.6957\n",
            "Epoch [120], val_loss: 186.6411\n",
            "Epoch [140], val_loss: 186.5854\n",
            "Epoch [160], val_loss: 186.5327\n",
            "Epoch [180], val_loss: 186.4768\n",
            "Epoch [200], val_loss: 186.4270\n",
            "Epoch [220], val_loss: 186.3643\n",
            "Epoch [240], val_loss: 186.3163\n",
            "Epoch [260], val_loss: 186.2599\n",
            "Epoch [280], val_loss: 186.2060\n",
            "Epoch [300], val_loss: 186.1461\n",
            "Epoch [320], val_loss: 186.0963\n",
            "Epoch [340], val_loss: 186.0421\n",
            "Epoch [360], val_loss: 185.9861\n",
            "Epoch [380], val_loss: 185.9261\n",
            "Epoch [400], val_loss: 185.8772\n",
            "Epoch [420], val_loss: 185.8196\n",
            "Epoch [440], val_loss: 185.7613\n",
            "Epoch [460], val_loss: 185.7101\n",
            "Epoch [480], val_loss: 185.6546\n",
            "Epoch [500], val_loss: 185.5990\n",
            "Epoch [520], val_loss: 185.5397\n",
            "Epoch [540], val_loss: 185.4913\n",
            "Epoch [560], val_loss: 185.4379\n",
            "Epoch [580], val_loss: 185.3851\n",
            "Epoch [600], val_loss: 185.3302\n",
            "Epoch [620], val_loss: 185.2753\n",
            "Epoch [640], val_loss: 185.2206\n",
            "Epoch [660], val_loss: 185.1633\n",
            "Epoch [680], val_loss: 185.1151\n",
            "Epoch [700], val_loss: 185.0535\n",
            "Epoch [720], val_loss: 185.0015\n",
            "Epoch [740], val_loss: 184.9444\n",
            "Epoch [760], val_loss: 184.8863\n",
            "Epoch [780], val_loss: 184.8345\n",
            "Epoch [800], val_loss: 184.7795\n",
            "Epoch [820], val_loss: 184.7296\n",
            "Epoch [840], val_loss: 184.6722\n",
            "Epoch [860], val_loss: 184.6194\n",
            "Epoch [880], val_loss: 184.5618\n",
            "Epoch [900], val_loss: 184.5083\n",
            "Epoch [920], val_loss: 184.4512\n",
            "Epoch [940], val_loss: 184.4033\n",
            "Epoch [960], val_loss: 184.3455\n",
            "Epoch [980], val_loss: 184.2902\n",
            "Epoch [1000], val_loss: 184.2354\n",
            "Epoch [1020], val_loss: 184.1773\n",
            "Epoch [1040], val_loss: 184.1240\n",
            "Epoch [1060], val_loss: 184.0713\n",
            "Epoch [1080], val_loss: 184.0115\n",
            "Epoch [1100], val_loss: 183.9618\n",
            "Epoch [1120], val_loss: 183.9050\n",
            "Epoch [1140], val_loss: 183.8495\n",
            "Epoch [1160], val_loss: 183.7964\n",
            "Epoch [1180], val_loss: 183.7472\n",
            "Epoch [1200], val_loss: 183.6867\n",
            "Epoch [1220], val_loss: 183.6335\n",
            "Epoch [1240], val_loss: 183.5778\n",
            "Epoch [1260], val_loss: 183.5208\n",
            "Epoch [1280], val_loss: 183.4606\n",
            "Epoch [1300], val_loss: 183.4109\n",
            "Epoch [1320], val_loss: 183.3554\n",
            "Epoch [1340], val_loss: 183.2995\n",
            "Epoch [1360], val_loss: 183.2477\n",
            "Epoch [1380], val_loss: 183.1962\n",
            "Epoch [1400], val_loss: 183.1399\n",
            "Epoch [1420], val_loss: 183.0863\n",
            "Epoch [1440], val_loss: 183.0302\n",
            "Epoch [1460], val_loss: 182.9798\n",
            "Epoch [1480], val_loss: 182.9254\n",
            "Epoch [1500], val_loss: 182.8663\n",
            "Epoch [1520], val_loss: 182.8187\n",
            "Epoch [1540], val_loss: 182.7592\n",
            "Epoch [1560], val_loss: 182.7063\n",
            "Epoch [1580], val_loss: 182.6420\n",
            "Epoch [1600], val_loss: 182.5964\n",
            "Epoch [1620], val_loss: 182.5380\n",
            "Epoch [1640], val_loss: 182.4876\n",
            "Epoch [1660], val_loss: 182.4318\n",
            "Epoch [1680], val_loss: 182.3760\n",
            "Epoch [1700], val_loss: 182.3251\n",
            "Epoch [1720], val_loss: 182.2669\n",
            "Epoch [1740], val_loss: 182.2030\n",
            "Epoch [1760], val_loss: 182.1493\n",
            "Epoch [1780], val_loss: 182.1017\n",
            "Epoch [1800], val_loss: 182.0497\n",
            "Epoch [1820], val_loss: 181.9890\n",
            "Epoch [1840], val_loss: 181.9367\n",
            "Epoch [1860], val_loss: 181.8836\n",
            "Epoch [1880], val_loss: 181.8297\n",
            "Epoch [1900], val_loss: 181.7774\n",
            "Epoch [1920], val_loss: 181.7230\n",
            "Epoch [1940], val_loss: 181.6627\n",
            "Epoch [1960], val_loss: 181.6096\n",
            "Epoch [1980], val_loss: 181.5504\n",
            "Epoch [2000], val_loss: 181.5005\n",
            "Epoch [2020], val_loss: 181.4456\n",
            "Epoch [2040], val_loss: 181.3930\n",
            "Epoch [2060], val_loss: 181.3379\n",
            "Epoch [2080], val_loss: 181.2810\n",
            "Epoch [2100], val_loss: 181.2278\n",
            "Epoch [2120], val_loss: 181.1717\n",
            "Epoch [2140], val_loss: 181.1169\n",
            "Epoch [2160], val_loss: 181.0627\n",
            "Epoch [2180], val_loss: 181.0107\n",
            "Epoch [2200], val_loss: 180.9569\n",
            "Epoch [2220], val_loss: 180.8994\n",
            "Epoch [2240], val_loss: 180.8426\n",
            "Epoch [2260], val_loss: 180.7930\n",
            "Epoch [2280], val_loss: 180.7375\n",
            "Epoch [2300], val_loss: 180.6827\n",
            "Epoch [2320], val_loss: 180.6270\n",
            "Epoch [2340], val_loss: 180.5693\n",
            "Epoch [2360], val_loss: 180.5191\n",
            "Epoch [2380], val_loss: 180.4643\n",
            "Epoch [2400], val_loss: 180.4095\n",
            "Epoch [2420], val_loss: 180.3490\n",
            "Epoch [2440], val_loss: 180.2995\n",
            "Epoch [2460], val_loss: 180.2407\n",
            "Epoch [2480], val_loss: 180.1879\n",
            "Epoch [2500], val_loss: 180.1358\n",
            "Epoch [2520], val_loss: 180.0779\n",
            "Epoch [2540], val_loss: 180.0272\n",
            "Epoch [2560], val_loss: 179.9721\n",
            "Epoch [2580], val_loss: 179.9178\n",
            "Epoch [2600], val_loss: 179.8598\n",
            "Epoch [2620], val_loss: 179.8049\n",
            "Epoch [2640], val_loss: 179.7490\n",
            "Epoch [2660], val_loss: 179.6925\n",
            "Epoch [2680], val_loss: 179.6454\n",
            "Epoch [2700], val_loss: 179.5872\n",
            "Epoch [2720], val_loss: 179.5309\n",
            "Epoch [2740], val_loss: 179.4755\n",
            "Epoch [2760], val_loss: 179.4228\n",
            "Epoch [2780], val_loss: 179.3710\n",
            "Epoch [2800], val_loss: 179.3157\n",
            "Epoch [2820], val_loss: 179.2601\n",
            "Epoch [2840], val_loss: 179.2070\n",
            "Epoch [2860], val_loss: 179.1518\n",
            "Epoch [2880], val_loss: 179.0924\n",
            "Epoch [2900], val_loss: 179.0446\n",
            "Epoch [2920], val_loss: 178.9880\n",
            "Epoch [2940], val_loss: 178.9316\n",
            "Epoch [2960], val_loss: 178.8736\n",
            "Epoch [2980], val_loss: 178.8236\n",
            "Epoch [3000], val_loss: 178.7635\n",
            "Epoch [3020], val_loss: 178.7088\n",
            "Epoch [3040], val_loss: 178.6590\n",
            "Epoch [3060], val_loss: 178.6077\n",
            "Epoch [3080], val_loss: 178.5501\n",
            "Epoch [3100], val_loss: 178.4899\n",
            "Epoch [3120], val_loss: 178.4445\n",
            "Epoch [3140], val_loss: 178.3871\n",
            "Epoch [3160], val_loss: 178.3318\n",
            "Epoch [3180], val_loss: 178.2802\n",
            "Epoch [3200], val_loss: 178.2245\n",
            "Epoch [3220], val_loss: 178.1702\n",
            "Epoch [3240], val_loss: 178.1116\n",
            "Epoch [3260], val_loss: 178.0585\n",
            "Epoch [3280], val_loss: 177.9998\n",
            "Epoch [3300], val_loss: 177.9473\n",
            "Epoch [3320], val_loss: 177.8931\n",
            "Epoch [3340], val_loss: 177.8352\n",
            "Epoch [3360], val_loss: 177.7835\n",
            "Epoch [3380], val_loss: 177.7282\n",
            "Epoch [3400], val_loss: 177.6793\n",
            "Epoch [3420], val_loss: 177.6252\n",
            "Epoch [3440], val_loss: 177.5673\n",
            "Epoch [3460], val_loss: 177.5119\n",
            "Epoch [3480], val_loss: 177.4486\n",
            "Epoch [3500], val_loss: 177.3941\n",
            "Epoch [3520], val_loss: 177.3466\n",
            "Epoch [3540], val_loss: 177.2935\n",
            "Epoch [3560], val_loss: 177.2423\n",
            "Epoch [3580], val_loss: 177.1829\n",
            "Epoch [3600], val_loss: 177.1302\n",
            "Epoch [3620], val_loss: 177.0700\n",
            "Epoch [3640], val_loss: 177.0211\n",
            "Epoch [3660], val_loss: 176.9631\n",
            "Epoch [3680], val_loss: 176.9102\n",
            "Epoch [3700], val_loss: 176.8522\n",
            "Epoch [3720], val_loss: 176.8013\n",
            "Epoch [3740], val_loss: 176.7475\n",
            "Epoch [3760], val_loss: 176.6835\n",
            "Epoch [3780], val_loss: 176.6316\n",
            "Epoch [3800], val_loss: 176.5855\n",
            "Epoch [3820], val_loss: 176.5261\n",
            "Epoch [3840], val_loss: 176.4720\n",
            "Epoch [3860], val_loss: 176.4161\n",
            "Epoch [3880], val_loss: 176.3625\n",
            "Epoch [3900], val_loss: 176.3072\n",
            "Epoch [3920], val_loss: 176.2553\n",
            "Epoch [3940], val_loss: 176.1968\n",
            "Epoch [3960], val_loss: 176.1422\n",
            "Epoch [3980], val_loss: 176.0914\n",
            "Epoch [4000], val_loss: 176.0350\n",
            "Epoch [4020], val_loss: 175.9805\n",
            "Epoch [4040], val_loss: 175.9240\n",
            "Epoch [4060], val_loss: 175.8615\n",
            "Epoch [4080], val_loss: 175.8100\n",
            "Epoch [4100], val_loss: 175.7633\n",
            "Epoch [4120], val_loss: 175.7084\n",
            "Epoch [4140], val_loss: 175.6498\n",
            "Epoch [4160], val_loss: 175.5959\n",
            "Epoch [4180], val_loss: 175.5389\n",
            "Epoch [4200], val_loss: 175.4929\n",
            "Epoch [4220], val_loss: 175.4336\n",
            "Epoch [4240], val_loss: 175.3804\n",
            "Epoch [4260], val_loss: 175.3249\n",
            "Epoch [4280], val_loss: 175.2686\n",
            "Epoch [4300], val_loss: 175.2174\n",
            "Epoch [4320], val_loss: 175.1599\n",
            "Epoch [4340], val_loss: 175.1080\n",
            "Epoch [4360], val_loss: 175.0484\n",
            "Epoch [4380], val_loss: 174.9908\n",
            "Epoch [4400], val_loss: 174.9368\n",
            "Epoch [4420], val_loss: 174.8830\n",
            "Epoch [4440], val_loss: 174.8342\n",
            "Epoch [4460], val_loss: 174.7719\n",
            "Epoch [4480], val_loss: 174.7244\n",
            "Epoch [4500], val_loss: 174.6643\n",
            "Epoch [4520], val_loss: 174.6163\n",
            "Epoch [4540], val_loss: 174.5603\n",
            "Epoch [4560], val_loss: 174.5050\n",
            "Epoch [4580], val_loss: 174.4476\n",
            "Epoch [4600], val_loss: 174.3955\n",
            "Epoch [4620], val_loss: 174.3443\n",
            "Epoch [4640], val_loss: 174.2872\n",
            "Epoch [4660], val_loss: 174.2297\n",
            "Epoch [4680], val_loss: 174.1734\n",
            "Epoch [4700], val_loss: 174.1226\n",
            "Epoch [4720], val_loss: 174.0672\n",
            "Epoch [4740], val_loss: 174.0096\n",
            "Epoch [4760], val_loss: 173.9621\n",
            "Epoch [4780], val_loss: 173.9068\n",
            "Epoch [4800], val_loss: 173.8500\n",
            "Epoch [4820], val_loss: 173.7949\n",
            "Epoch [4840], val_loss: 173.7391\n",
            "Epoch [4860], val_loss: 173.6847\n",
            "Epoch [4880], val_loss: 173.6244\n",
            "Epoch [4900], val_loss: 173.5766\n",
            "Epoch [4920], val_loss: 173.5177\n",
            "Epoch [4940], val_loss: 173.4560\n",
            "Epoch [4960], val_loss: 173.4067\n",
            "Epoch [4980], val_loss: 173.3485\n",
            "Epoch [5000], val_loss: 173.2963\n",
            "Epoch [5020], val_loss: 173.2441\n",
            "Epoch [5040], val_loss: 173.1940\n",
            "Epoch [5060], val_loss: 173.1347\n",
            "Epoch [5080], val_loss: 173.0812\n",
            "Epoch [5100], val_loss: 173.0236\n",
            "Epoch [5120], val_loss: 172.9681\n",
            "Epoch [5140], val_loss: 172.9175\n",
            "Epoch [5160], val_loss: 172.8670\n",
            "Epoch [5180], val_loss: 172.8114\n",
            "Epoch [5200], val_loss: 172.7554\n",
            "Epoch [5220], val_loss: 172.6988\n",
            "Epoch [5240], val_loss: 172.6381\n",
            "Epoch [5260], val_loss: 172.5853\n",
            "Epoch [5280], val_loss: 172.5299\n",
            "Epoch [5300], val_loss: 172.4787\n",
            "Epoch [5320], val_loss: 172.4209\n",
            "Epoch [5340], val_loss: 172.3698\n",
            "Epoch [5360], val_loss: 172.3204\n",
            "Epoch [5380], val_loss: 172.2622\n",
            "Epoch [5400], val_loss: 172.2000\n",
            "Epoch [5420], val_loss: 172.1525\n",
            "Epoch [5440], val_loss: 172.0978\n",
            "Epoch [5460], val_loss: 172.0408\n",
            "Epoch [5480], val_loss: 171.9818\n",
            "Epoch [5500], val_loss: 171.9314\n",
            "Epoch [5520], val_loss: 171.8781\n",
            "Epoch [5540], val_loss: 171.8209\n",
            "Epoch [5560], val_loss: 171.7674\n",
            "Epoch [5580], val_loss: 171.7149\n",
            "Epoch [5600], val_loss: 171.6623\n",
            "Epoch [5620], val_loss: 171.6053\n",
            "Epoch [5640], val_loss: 171.5462\n",
            "Epoch [5660], val_loss: 171.4926\n",
            "Epoch [5680], val_loss: 171.4390\n",
            "Epoch [5700], val_loss: 171.3814\n",
            "Epoch [5720], val_loss: 171.3298\n",
            "Epoch [5740], val_loss: 171.2696\n",
            "Epoch [5760], val_loss: 171.2137\n",
            "Epoch [5780], val_loss: 171.1689\n",
            "Epoch [5800], val_loss: 171.1140\n",
            "Epoch [5820], val_loss: 171.0595\n",
            "Epoch [5840], val_loss: 171.0018\n",
            "Epoch [5860], val_loss: 170.9490\n",
            "Epoch [5880], val_loss: 170.8945\n",
            "Epoch [5900], val_loss: 170.8419\n",
            "Epoch [5920], val_loss: 170.7861\n",
            "Epoch [5940], val_loss: 170.7330\n",
            "Epoch [5960], val_loss: 170.6735\n",
            "Epoch [5980], val_loss: 170.6237\n",
            "Epoch [6000], val_loss: 170.5633\n",
            "Epoch [6020], val_loss: 170.5080\n",
            "Epoch [6040], val_loss: 170.4582\n",
            "Epoch [6060], val_loss: 170.4005\n",
            "Epoch [6080], val_loss: 170.3493\n",
            "Epoch [6100], val_loss: 170.2923\n",
            "Epoch [6120], val_loss: 170.2404\n",
            "Epoch [6140], val_loss: 170.1792\n",
            "Epoch [6160], val_loss: 170.1241\n",
            "Epoch [6180], val_loss: 170.0764\n",
            "Epoch [6200], val_loss: 170.0157\n",
            "Epoch [6220], val_loss: 169.9638\n",
            "Epoch [6240], val_loss: 169.9074\n",
            "Epoch [6260], val_loss: 169.8598\n",
            "Epoch [6280], val_loss: 169.7998\n",
            "Epoch [6300], val_loss: 169.7463\n",
            "Epoch [6320], val_loss: 169.6924\n",
            "Epoch [6340], val_loss: 169.6378\n",
            "Epoch [6360], val_loss: 169.5784\n",
            "Epoch [6380], val_loss: 169.5265\n",
            "Epoch [6400], val_loss: 169.4726\n",
            "Epoch [6420], val_loss: 169.4228\n",
            "Epoch [6440], val_loss: 169.3695\n",
            "Epoch [6460], val_loss: 169.3047\n",
            "Epoch [6480], val_loss: 169.2553\n",
            "Epoch [6500], val_loss: 169.2013\n",
            "Epoch [6520], val_loss: 169.1435\n",
            "Epoch [6540], val_loss: 169.0882\n",
            "Epoch [6560], val_loss: 169.0343\n",
            "Epoch [6580], val_loss: 168.9813\n",
            "Epoch [6600], val_loss: 168.9293\n",
            "Epoch [6620], val_loss: 168.8737\n",
            "Epoch [6640], val_loss: 168.8119\n",
            "Epoch [6660], val_loss: 168.7662\n",
            "Epoch [6680], val_loss: 168.7060\n",
            "Epoch [6700], val_loss: 168.6536\n",
            "Epoch [6720], val_loss: 168.6003\n",
            "Epoch [6740], val_loss: 168.5376\n",
            "Epoch [6760], val_loss: 168.4919\n",
            "Epoch [6780], val_loss: 168.4365\n",
            "Epoch [6800], val_loss: 168.3780\n",
            "Epoch [6820], val_loss: 168.3260\n",
            "Epoch [6840], val_loss: 168.2698\n",
            "Epoch [6860], val_loss: 168.2122\n",
            "Epoch [6880], val_loss: 168.1616\n",
            "Epoch [6900], val_loss: 168.1020\n",
            "Epoch [6920], val_loss: 168.0486\n",
            "Epoch [6940], val_loss: 167.9966\n",
            "Epoch [6960], val_loss: 167.9448\n",
            "Epoch [6980], val_loss: 167.8863\n",
            "Epoch [7000], val_loss: 167.8233\n",
            "Epoch [7020], val_loss: 167.7712\n",
            "Epoch [7040], val_loss: 167.7206\n",
            "Epoch [7060], val_loss: 167.6698\n",
            "Epoch [7080], val_loss: 167.6177\n",
            "Epoch [7100], val_loss: 167.5550\n",
            "Epoch [7120], val_loss: 167.5007\n",
            "Epoch [7140], val_loss: 167.4459\n",
            "Epoch [7160], val_loss: 167.3955\n",
            "Epoch [7180], val_loss: 167.3395\n",
            "Epoch [7200], val_loss: 167.2841\n",
            "Epoch [7220], val_loss: 167.2258\n",
            "Epoch [7240], val_loss: 167.1678\n",
            "Epoch [7260], val_loss: 167.1144\n",
            "Epoch [7280], val_loss: 167.0672\n",
            "Epoch [7300], val_loss: 167.0099\n",
            "Epoch [7320], val_loss: 166.9583\n",
            "Epoch [7340], val_loss: 166.9039\n",
            "Epoch [7360], val_loss: 166.8487\n",
            "Epoch [7380], val_loss: 166.7924\n",
            "Epoch [7400], val_loss: 166.7384\n",
            "Epoch [7420], val_loss: 166.6805\n",
            "Epoch [7440], val_loss: 166.6274\n",
            "Epoch [7460], val_loss: 166.5795\n",
            "Epoch [7480], val_loss: 166.5192\n",
            "Epoch [7500], val_loss: 166.4676\n",
            "Epoch [7520], val_loss: 166.4148\n",
            "Epoch [7540], val_loss: 166.3563\n",
            "Epoch [7560], val_loss: 166.3035\n",
            "Epoch [7580], val_loss: 166.2466\n",
            "Epoch [7600], val_loss: 166.1893\n",
            "Epoch [7620], val_loss: 166.1358\n",
            "Epoch [7640], val_loss: 166.0842\n",
            "Epoch [7660], val_loss: 166.0279\n",
            "Epoch [7680], val_loss: 165.9707\n",
            "Epoch [7700], val_loss: 165.9199\n",
            "Epoch [7720], val_loss: 165.8626\n",
            "Epoch [7740], val_loss: 165.8006\n",
            "Epoch [7760], val_loss: 165.7469\n",
            "Epoch [7780], val_loss: 165.6963\n",
            "Epoch [7800], val_loss: 165.6404\n",
            "Epoch [7820], val_loss: 165.5918\n",
            "Epoch [7840], val_loss: 165.5374\n",
            "Epoch [7860], val_loss: 165.4803\n",
            "Epoch [7880], val_loss: 165.4260\n",
            "Epoch [7900], val_loss: 165.3734\n",
            "Epoch [7920], val_loss: 165.3177\n",
            "Epoch [7940], val_loss: 165.2597\n",
            "Epoch [7960], val_loss: 165.2038\n",
            "Epoch [7980], val_loss: 165.1497\n",
            "Epoch [8000], val_loss: 165.1010\n",
            "Epoch [8020], val_loss: 165.0453\n",
            "Epoch [8040], val_loss: 164.9877\n",
            "Epoch [8060], val_loss: 164.9273\n",
            "Epoch [8080], val_loss: 164.8749\n",
            "Epoch [8100], val_loss: 164.8268\n",
            "Epoch [8120], val_loss: 164.7718\n",
            "Epoch [8140], val_loss: 164.7187\n",
            "Epoch [8160], val_loss: 164.6603\n",
            "Epoch [8180], val_loss: 164.6022\n",
            "Epoch [8200], val_loss: 164.5517\n",
            "Epoch [8220], val_loss: 164.4991\n",
            "Epoch [8240], val_loss: 164.4426\n",
            "Epoch [8260], val_loss: 164.3888\n",
            "Epoch [8280], val_loss: 164.3331\n",
            "Epoch [8300], val_loss: 164.2780\n",
            "Epoch [8320], val_loss: 164.2265\n",
            "Epoch [8340], val_loss: 164.1645\n",
            "Epoch [8360], val_loss: 164.1139\n",
            "Epoch [8380], val_loss: 164.0605\n",
            "Epoch [8400], val_loss: 164.0080\n",
            "Epoch [8420], val_loss: 163.9474\n",
            "Epoch [8440], val_loss: 163.8957\n",
            "Epoch [8460], val_loss: 163.8456\n",
            "Epoch [8480], val_loss: 163.7865\n",
            "Epoch [8500], val_loss: 163.7278\n",
            "Epoch [8520], val_loss: 163.6711\n",
            "Epoch [8540], val_loss: 163.6167\n",
            "Epoch [8560], val_loss: 163.5636\n",
            "Epoch [8580], val_loss: 163.5114\n",
            "Epoch [8600], val_loss: 163.4506\n",
            "Epoch [8620], val_loss: 163.3992\n",
            "Epoch [8640], val_loss: 163.3545\n",
            "Epoch [8660], val_loss: 163.2942\n",
            "Epoch [8680], val_loss: 163.2401\n",
            "Epoch [8700], val_loss: 163.1791\n",
            "Epoch [8720], val_loss: 163.1237\n",
            "Epoch [8740], val_loss: 163.0655\n",
            "Epoch [8760], val_loss: 163.0182\n",
            "Epoch [8780], val_loss: 162.9730\n",
            "Epoch [8800], val_loss: 162.9151\n",
            "Epoch [8820], val_loss: 162.8587\n",
            "Epoch [8840], val_loss: 162.8046\n",
            "Epoch [8860], val_loss: 162.7452\n",
            "Epoch [8880], val_loss: 162.6885\n",
            "Epoch [8900], val_loss: 162.6341\n",
            "Epoch [8920], val_loss: 162.5806\n",
            "Epoch [8940], val_loss: 162.5252\n",
            "Epoch [8960], val_loss: 162.4718\n",
            "Epoch [8980], val_loss: 162.4221\n",
            "Epoch [9000], val_loss: 162.3643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction Algorithm\n",
        "def predict_single(input, target, model):\n",
        "    inputs = input.unsqueeze(0)\n",
        "    predictions = model(inputs)                # fill this\n",
        "    prediction = predictions[0].detach()\n",
        "    print(\"Input:\", input)\n",
        "    print(\"Target:\", target)\n",
        "    print(\"Prediction:\", prediction)\n",
        "\n",
        "# Testing the model with some samples\n",
        "input, target = val_ds[0]\n",
        "predict_single(input, target, model)"
      ],
      "metadata": {
        "id": "Df9Roh3kWIR_",
        "outputId": "11224d1e-f217-4719-c2a8-b9b7a93d3bf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: tensor([2.1953e+03, 8.0600e+00, 4.5780e+04, 0.0000e+00])\n",
            "Target: tensor([5.3350])\n",
            "Prediction: tensor([-22.0927])\n"
          ]
        }
      ]
    }
  ]
}